import gzip as gzip
import itertools
import os
import random
import numpy as np
import src.utils.utils as utils
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import MultinomialNB

global db
global collection



def get_file(path):
    """ Returns a single read bytecode file as a list of lists
    Args:
        path (string): full file path with filename and extension
    """
    with gzip.open(path+'.bytes.gz', 'rb') as f:
        # strips whitespace
        return [bytes.decode(line.strip()) for line in f.readlines()]

def stream_files(collection, test=False):
    """
    Generator function to return a tuple of information about a file
    """
    if test:
        path = '/malware/data/test/'
        data = collection.find({'rand_id': {'$exists': True}},
                no_cursor_timeout=True)
    else:
        path = '/malware/data/train/'
        classes = [str(i) for i in range(10)]
        # required for keeping the cursor from timing out
        data = collection.find({'rand_id': {'$exists': True}},
                no_cursor_timeout=True)

    # number of classes in our data
    print(data)
    for i, doc in enumerate(data):
        if test:
            yield doc['id'], get_file(path + doc['id'])
        else:
            if i % 100 == 0:
                print('%d, about to get %s in class %s' % (i, doc['id'], doc['class']))
            yield doc['id'], doc['class'], get_file(path + doc['id'])

def make_vectorizer():
    """
    Utility function to get back the vectorizer we want
    """
    return HashingVectorizer(decode_error='ignore',
            n_features=2**27,
            non_negative=True,
            analyzer='word',
            token_pattern=' ',
            ngram_range=(2,4)
            )


def get_batch(mongo_iter, size):
    # limit size?

    data = [doc for doc in itertools.islice(mongo_iter, size)]
    X_id, X_lab, X_bytes = zip(*data)
    print("id %d, class, %d, data size %d" % (len(X_id), len(X_lab), len(X_bytes)))
    return X_bytes, np.asarray(X_lab, dtype=int)

def gen_batch(mongo_iter, batch_size=10):
    """
    Generator to return a batch of data
    """
    X_bytes, y = get_batch(mongo_iter, batch_size)
    while len(X_bytes):
        yield X_bytes, y
        X_bytes, y = get_batch(mongo_iter, batch_size)


def set_collection(col):
    if collection:
        collection = col
    else:
        print('non initialized collection; setting it to training set')
        init('train')


def init(sample):
    db = utils.get_mongodb()
    if sample == 'train':
        collection = db.samples
    if sample == 'test':
        collection = db.test_samples


def run(sample, testsize):
    doc_stream = stream_files()
    X_train_test, y_train_test = get_batch(doc_stream, testsize)
    h_vectorizer = make_vectorizer()
    bow_train_test = h_vectorizer.transform(X_train_test)
    print("Test set is %d documents " % (len(y_train_test),
        sum(y_train_test)))
