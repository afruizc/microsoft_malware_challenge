import gzip as gzip
import itertools
import os
import numpy as np
import src.utils.utils as utils
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.linear_model import SGDClassifier
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.linear_model import Perceptron
from sklearn.naive_bayes import MultinomialNB

def get_file(path):
    """ Returns a single read bytecode file as a list of lists
    Args:
        path (string): full file path with filename and extension
    """
    with gzip.open(path+'.bytes.gz', 'rb') as f:
        # strips whitespace
        return [line.strip() for line in f.readlines()]

def stream_files(collection, test=False):
    """
    Generator function to return a tuple of information about a file
    """
    if test:
        path = '/malware/data/test/'
    else:
        path = '/malware/data/train/'

    # number of classes in our data
    classes = [str(i) for i in range(10)]
    data = collection.find({'class': {'$in': classes}})
    print(data)
    for doc in data:
        print('about to get %s' % doc['id'])
        yield doc['id'], doc['class'], get_file(path + doc['id'])

def make_vectorizer():
    """
    Utility function to get back the vectorizer we want
    """
    return HashingVectorizer(decode_error='ignore',
            n_features=2**18,
            non_negative=True,
            analyzer='word',
            encoding='bytes')


def get_batch(mongo_iter, size):
    # limit size?

    data = [doc for doc in itertools.islice(mongo_iter, size)]
    X_id, X_lab, X_bytes = zip(*data)
    print("id %d, class, %d, data size %d" % (len(X_id), len(X_lab), len(X_bytes)))
    return X_bytes, np.asarray(X_lab, dtype=int)

def gen_batch(mongo_iter, batch_size=10):
    """
    Generator to return a batch of data
    """
    X_bytes, y = get_batch(mongo_iter, batch_size)
    while len(X_bytes):
        yield X_bytes, y
        X_bytes, y = get_batch(mongo_iter, batch_size)




def run(sample, testsize):
    global db
    global collection
    db = utils.get_mongodb()
    if sample == 'train':
        collection = db.samples
    if sample == 'train':
        collection = db.test_samples
    doc_stream = stream_files()
    X_train_test, y_train_test = get_batch(doc_stream, testsize)
    h_vectorizer = make_vectorizer()
    bow_train_test = h_vectorizer.transform(X_train_test)
    print("Test set is %d documents " % (len(y_train_test),
        sum(y_train_test)))


    





